import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import sys

# è·¯å¾„ hack
current_dir = os.path.dirname(os.path.abspath(__file__))
core_path = os.path.join(current_dir, "core")
if core_path not in sys.path: sys.path.insert(0, core_path)

# ==============================================================================
# å¤ç”¨æ¨¡å‹å®šä¹‰ (å¿…é¡»ä¸ train_sim.py è¿™é‡Œçš„ 2å±‚å°æ¨¡å‹ ä¸€è‡´)
# ==============================================================================
class CausalTransformer(nn.Module):
    def __init__(self, input_dim, output_dim, embed_dim=64, num_layers=2, num_heads=4, history_len=10):
        super().__init__()
        self.embedding = nn.Linear(input_dim, embed_dim)
        self.pos_embed = nn.Parameter(torch.zeros(1, history_len, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.0, activation='relu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.head = nn.Linear(embed_dim, output_dim)

    def forward(self, x):
        B, T, D = x.shape
        emb = self.embedding(x) + self.pos_embed[:, :T, :]
        mask = torch.triu(torch.ones(T, T) * float('-inf'), diagonal=1).to(x.device)
        feat = self.transformer(emb, mask=mask)
        return self.head(feat[:, -1, :])

# ==============================================================================
# Real Dataset (é€‚é… 1-DOF -> 2-DOF æ˜ å°„)
# ==============================================================================
class RealDataset(Dataset):
    def __init__(self, npz_path, stats_path, history_len=10):
        # 1. åŠ è½½å®ç‰©æ•°æ®
        data = np.load(npz_path)
        # ç¡®ä¿æ•°æ®æ˜¯ Float32
        q_real = torch.from_numpy(data['q']).float()
        qd_real = torch.from_numpy(data['qd']).float()
        self.tau = torch.from_numpy(data['tau']).float()
        
        # 2. åŠ è½½ Sim ç»Ÿè®¡é‡
        if not os.path.exists(stats_path):
            raise FileNotFoundError("æ‰¾ä¸åˆ° nerd_stats.ptï¼")
        self.stats = torch.load(stats_path, map_location='cpu')
        
        # 3. ç‰¹å¾å·¥ç¨‹ + ç»´åº¦å¡«å……
        # Sim Feat: [sin0, sin1, cos0, cos1, vel0, vel1]
        # Real Data: Only 0 (Motor)
        # ç­–ç•¥: å‡è®¾ sin1=sin0, cos1=cos0, vel1=vel0 (åˆå§‹çŒœæµ‹)
        
        sin_q = torch.sin(q_real)
        cos_q = torch.cos(q_real)
        
        self.feat_states = torch.cat([
            sin_q, sin_q,   # sin
            cos_q, cos_q,   # cos
            qd_real, qd_real # vel
        ], dim=-1) # [N, 6]
        
        # 4. å½’ä¸€åŒ– (ä½¿ç”¨ Sim çš„æ ‡å‡†)
        self.norm_states = (self.feat_states - self.stats['s_mean']) / self.stats['s_std']
        self.norm_actions = (self.tau - self.stats['a_mean']) / self.stats['a_std']
        
        # 5. å‡†å¤‡ Target
        # æˆ‘ä»¬åªèƒ½è®¡ç®— Motor çš„ Delta
        # Sim Target Delta: [dq0, dq1, dv0, dv1]
        raw_dq = q_real[1:] - q_real[:-1]
        raw_dv = qd_real[1:] - qd_real[:-1]
        
        # å½’ä¸€åŒ– Delta
        d_mean = self.stats['d_mean']
        d_std = self.stats['d_std']
        
        # æ„é€ å…¨é›¶ Targetï¼Œç¨ååªè®­ç»ƒæœ‰æ•°æ®çš„ç»´åº¦
        self.norm_targets = torch.zeros(len(raw_dq), 4)
        
        # å¡«å…… Motor Pos Delta (Index 0)
        self.norm_targets[:, 0] = (raw_dq.squeeze() - d_mean[0]) / d_std[0]
        # å¡«å…… Motor Vel Delta (Index 2)
        self.norm_targets[:, 2] = (raw_dv.squeeze() - d_mean[2]) / d_std[2]
        
        self.h = history_len
        self.length = len(self.norm_states) - self.h - 1

    def __len__(self):
        return max(0, self.length)

    def __getitem__(self, idx):
        # Input
        s_win = self.norm_states[idx : idx+self.h]
        a_win = self.norm_actions[idx : idx+self.h]
        inputs = torch.cat([s_win, a_win], dim=-1) # [h, 7]
        
        # Target
        target = self.norm_targets[idx + self.h - 1] # [4]
        
        # Mask: [1, 0, 1, 0] -> åªç›‘ç£ Motor Pos å’Œ Motor Vel
        mask = torch.tensor([1.0, 0.0, 1.0, 0.0])
        
        return inputs, target, mask

def finetune():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("=== Sim-to-Real Fine-tuning ===")
    
    # 1. ç»´åº¦é…ç½®
    # In: 6(feat) + 1(act) = 7
    # Out: 4(delta)
    model = CausalTransformer(
        input_dim=7, output_dim=4, 
        embed_dim=64, num_layers=2, num_heads=4,
        history_len=10
    ).to(device)
    
    # 2. åŠ è½½æƒé‡
    if os.path.exists("nerd_sim_weights.pth"):
        model.load_state_dict(torch.load("nerd_sim_weights.pth"))
        print("âœ… Sim æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("âŒ æƒé‡ä¸¢å¤±")
        return

    # 3. å‡†å¤‡æ•°æ®
    if not os.path.exists("data/real_data.npz"):
        print("âš ï¸ ç”Ÿæˆæµ‹è¯•ç”¨å‡å®ç‰©æ•°æ®...")
        os.makedirs("data", exist_ok=True)
        N = 2000
        t = np.linspace(0, 10, N)
        # åŠ ä¸€ç‚¹ Sim ä¸å…·å¤‡çš„ç‰¹æ€§ (æ¯”å¦‚æ‘©æ“¦åŠ›æ›´å¤§å¯¼è‡´å¹…åº¦å˜å°)
        q = 0.8 * np.sin(t) 
        qd = 0.8 * np.cos(t)
        tau = np.sin(t)
        np.savez("data/real_data.npz", q=q[:,None], qd=qd[:,None], tau=tau[:,None])
        
    dataset = RealDataset("data/real_data.npz", "nerd_stats.pt")
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # 4. å¾®è°ƒ (Low LR)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.MSELoss(reduction='none') 
    
    model.train()
    print("å¼€å§‹å¾®è°ƒ...")
    for epoch in range(20):
        total_loss = 0
        steps = 0
        for x, y, mask in loader:
            x, y, mask = x.to(device), y.to(device), mask.to(device)
            
            optimizer.zero_grad()
            pred = model(x)
            
            # Masked Loss
            loss_all = criterion(pred, y)
            loss = (loss_all * mask).sum() / mask.sum()
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            steps += 1
            
        print(f"Finetune Epoch {epoch+1} | Loss: {total_loss/steps:.6f}")

    torch.save(model.state_dict(), "nerd_real_final.pth")
    print("ğŸ‰ All Done! æ¨¡å‹å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    finetune()